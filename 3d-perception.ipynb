{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Perception Project Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the Udacity Robotics Engineer Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Harry Turner on 5th March"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This write up explains the perception pipeline in detail. It also displays the results of testing the pipeline on three different test worlds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It address each of the points in the rubric, and offers supporting images and statements in each section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perception Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Point Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input from the RGBD camera is a point cloud, where every point has XYZ coordinates, and RGB colour information. Before I do any processing, I will visualise the point cloud without any noise, to see what I'm working with. This is, effectively, what the robot is \"seeing\".\n",
    "\n",
    "I used rViz to visualise the point cloud, the output is shown in the image below.\n",
    "\n",
    "You can see a selection of objects on the table, the resolution is so small that you can't even see the points. But this give me a pretty good idea of the sort of objects that my robot will need to recognise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![raw.png](images/raw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual input from the RGBD camera is noisy, this is important to recognise as it can be very detrimental to further processing techniques if not handled properly! \n",
    "\n",
    "I visualised the noisy point cloud in rViz, note that the points are represented as spheres now, to help see the noisy points.\n",
    "\n",
    "You can see a significant amount of noise surrounding the objects on the table! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![noise.png](images/noise.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise filter is important as it removes the noise from the input by treating them as statistical outliers. The noise filter itself is a statistical outlier filter that \"finds\" outliers and removes them. To find any outlier, it goes through every point and looks to see whether it's a certain distance away from it's neighbours.\n",
    "\n",
    "I implemented the noise filter using the PCL library, demonstrated in the code below. I set the values of k, and x by experiment. I found that these values reduced the amount of noise without losing any detail on the objects.\n",
    "\n",
    "The output from the noise filter is a point cloud with the noise removed. This can be seen in the image below. You can see that noise has been reduced significantly, this will make further processing steps easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Statistical Outlier Filtering\n",
    "    ###############################\n",
    "\n",
    "    # Create a outlier filter object for our input point cloud.\n",
    "    outlier_filter = cloud.make_statistical_outlier_filter()\n",
    "\n",
    "    # Set the number of neighboring points to analyze for any given point\n",
    "    outlier_filter.set_mean_k(10)\n",
    "\n",
    "    # Set threshold scale factor\n",
    "    x = 0.01\n",
    "\n",
    "    # Any point with a mean distance larger than global (mean distance+x*std_dev) will be considered outlier\n",
    "    outlier_filter.set_std_dev_mul_thresh(x)\n",
    "\n",
    "    # Finally call the filter function for magic\n",
    "    cloud_filtered = outlier_filter.filter() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![no_noise.png](images/no_noise.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxel Downsizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxel Downsizing is a method of reducing the number of points in the point cloud, by reducing the resolution of the points. The importance of this is that it significantly speeds up future processing steps! \n",
    "\n",
    "To achieve this, I implemented a voxel downsizing filter using the PCL library. The code for this implementation is shown below. I chose a leaf size of 0.01. This was found by experimentation to be the value that reduced the number of points significantly, whilst not loosing any important features.\n",
    "\n",
    "The result is shown in an image at the end of the \"RANSAC Filter\" section. You can see from the image that the resolution of the objects is much reduced, although they are still clearly identifiable which is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Voxel Grid filter\n",
    "    ###################\n",
    "\n",
    "    # Create a VoxelGrid filter object for our input point cloud.\n",
    "    vox = cloud_filtered.make_voxel_grid_filter()\n",
    "\n",
    "    # Leaf size.\n",
    "    LEAF_SIZE = 0.01\n",
    "\n",
    "    # Set the leaf size.\n",
    "    vox.set_leaf_size(LEAF_SIZE, LEAF_SIZE, LEAF_SIZE)\n",
    "\n",
    "    # Call the filter function to obtain the resultant downsampled point.\n",
    "    cloud_filtered = vox.filter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass Through Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pass through filter is a method of specifying a region of interest. All of the objects are on top of a table, so we don't care about anything under the table. Therefore, we can set a region of interest to be above the table, filtering out everything else. This is important for reducing the search space for later techniques. I found that I needed another filter to remove the boxes, which can actually be seen in the images above. Otherwise, they would confuse the object recognition algorithm.\n",
    "\n",
    "I implemented two filters, one in the Z axis, to filter out points below the table, and one in the X axis, to filter out the edges of the boxes. The values for the min and max were found by inspecting the scene and determining what region the objects of interest were in.\n",
    "\n",
    "The results are shown in the image at the end of the \"RANSAC Filter\" section. You can see from this image that the box edges have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # PassThrough filter Z\n",
    "    ######################\n",
    "\n",
    "    # Create a PassThrough filter object.\n",
    "    passthrough = cloud_filtered.make_passthrough_filter()\n",
    "\n",
    "    # Assign axis and range to the passthrough filter object.\n",
    "    filter_axis = 'z'\n",
    "    passthrough.set_filter_field_name(filter_axis)\n",
    "    axis_min = 0.6\n",
    "    axis_max = 0.8\n",
    "    passthrough.set_filter_limits(axis_min, axis_max)\n",
    "\n",
    "    # Use the filter function to obtain the resultant point cloud.\n",
    "    cloud_filtered = passthrough.filter()\n",
    "\n",
    "\n",
    "    # PassThrough filter X\n",
    "    ######################\n",
    "\n",
    "    # Create a PassThrough filter object.\n",
    "    passthrough = cloud_filtered.make_passthrough_filter()\n",
    "\n",
    "    # Assign axis and range to the passthrough filter object.\n",
    "    filter_axis = 'x'\n",
    "    passthrough.set_filter_field_name(filter_axis)\n",
    "    axis_min = 0.35\n",
    "    axis_max = 0.8\n",
    "    passthrough.set_filter_limits(axis_min, axis_max)\n",
    "\n",
    "    # Use the filter function to obtain the resultant point cloud.\n",
    "    cloud_filtered = passthrough.filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANSAC Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RANSAC filter is a method to find objects that meet a certain model. In this instance, I am using it to find objects that belong to a plane, because they are most likely to belong to the table! Therefore, I can find the table in the point cloud, by using RANSAC plane segmentation. This is important, because I can then remove the plane, and keep the objects behind, and the resulting point cloud will contain just the objects.\n",
    "\n",
    "I implemented the RANSAC plane segmentation with the PCL library, the code can be seen below. I found the value of max_distance by inspection and experimentation.\n",
    "\n",
    "The results can be seen in the image that follows the code. You can see three things here. First, the resolution is much reduced from the voxel downsizing. Secondly, the boxes have been removed from the pass through filter. Thirdly, the table has been removed by the RANSAC Filter. The objects are the only things left in the point cloud, which is now ready for clustering segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # RANSAC plane segmentation\n",
    "    ###########################\n",
    "\n",
    "    # Create the segmentation object.\n",
    "    seg = cloud_filtered.make_segmenter()\n",
    "\n",
    "    # Set the model you wish to fit.\n",
    "    seg.set_model_type(pcl.SACMODEL_PLANE)\n",
    "    seg.set_method_type(pcl.SAC_RANSAC)\n",
    "\n",
    "    # Max distance for a point to be considered fitting the model.\n",
    "    MAX_DISTANCE = 0.01\n",
    "    seg.set_distance_threshold(MAX_DISTANCE)\n",
    "\n",
    "    # Call the segment function.\n",
    "    inliers, coefficients = seg.segment()\n",
    "    \n",
    "    # Extract objects\n",
    "    #################\n",
    "\n",
    "    # Extract outliers\n",
    "    cloud_objects = cloud_filtered.extract(inliers, negative=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![objects.png](images/objects.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a method of grouping similar sets of points. In this case, I use the DBSCAN clustering algorithm, to find clusters of points within the point cloud, based on their XYZ values. The idea is that the clusters each represent an object, so by clustering the point cloud, I can segment it to find the objects. This is important so that I can use an object recognition algorithm on each of the clusters/objects in the next step.\n",
    "\n",
    "I implemented clustering using the PCL library, the code for this can be seen below. First I had to convert the points to XYZ, and then convert the points to a K-D tree. This is to make the algorithm more efficient by searching for neighbouring points. I found the values of cluster tolerance, minimum cluster size, and maximum cluster size by experiment. Since my objects were the only things in the scene, the largest cluster size needed to be larger than the number of points in the largest object. \n",
    "\n",
    "The results can be seen in the image below the code. In this case, each cluster has been coloured, so you can see that the clustering algorithm has identified the clusters to be the individual objects. Therefore the clustering algorithm has been successful, I can now extract the points for each cluster and pass these to the object recognition algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Euclidean Clustering\n",
    "    ######################\n",
    "\n",
    "    # Turn to XYZ\n",
    "    white_cloud = XYZRGB_to_XYZ(cloud_objects)\n",
    "\n",
    "    # Construct K-D Tree\n",
    "    tree = white_cloud.make_kdtree()\n",
    "\n",
    "    # Create cluster extraction object.\n",
    "    ec = white_cloud.make_EuclideanClusterExtraction()\n",
    "\n",
    "    # Set cluster parameters\n",
    "    ec.set_ClusterTolerance(0.05)\n",
    "    ec.set_MinClusterSize(10)\n",
    "    ec.set_MaxClusterSize(5000)\n",
    "\n",
    "    # Search the K-D Tree for clusters\n",
    "    ec.set_SearchMethod(tree)\n",
    "\n",
    "    # Extract indices for each of the discovered clusters\n",
    "    cluster_indices = ec.Extract()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![clusters.png](images/clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection is where I attempt to classify the clusters as a particular class of object. To do this, I'll use a machine learning algorithm called a Support Vector Machine. This requires training on a training set of objects. Once it's trained, it can be deployed in the perception pipeline. The features I'll input to the SVM are the normal signature for the cluster, and the colour signature. The signature in this case will be the histogram, constructed by finding the total distribution of colours and normals, and converting to them to histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train my SVM, I needed training data. \n",
    "\n",
    "I ran the \"capture_features.py\" script, with the number of instances per object set to 30. I also specified a HSV colour space for the histogram. Having 30 examples of each object, meant that my model would see the same object 30 different times, in different orientations, therefore it would hopefully learn robust features for that object. I used the HSV colour space as it's less sensitive to light. I then trained my model with the \"train_svm.py\" script, and outputted the results of it's accuracy, in the form of a confusion matrix. This can be seen below.\n",
    "\n",
    "The confusion matrix shows that the classifier is doing a good job of classifying the test data, as most of the weight falls along the diagonal. It gets the occasional classification wrong, for example, it has misclassified the glue for the soap. Overall, this classifier is strong enough to implement in my object detection pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure_1.png](images/figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection is the step where my SVM model attempts to classify the extracted objects from the clustering algorithm in the step before. This is important, because if it can classify the clusters as belonging to a particular class, then the robot has identified an object!\n",
    "\n",
    "To achieve this, I use my model to predict a class, using SKLEARN. I do this for every cluster in the list, and add each of the \"found\" objects to a new list. This new list is then output as the result.\n",
    "\n",
    "To see the results of the object detection, there is a line of code that outputs a label for each cluster, and publishes it to a topic. This label can then be visualised in rViz. In the image that follows the code, you can see the labels added to each of the clusters for the objects in the scene. It can be tricky to determine which label goes with which object, but you can see that the detection algorithm correctly classifies every object with the exception of the sticky notes, which it thinks are snacks. This is a successful implementation of the detection algorithm, and is ready to be deployed in the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Detect Objects\n",
    "    ################\n",
    "\n",
    "    detected_objects_labels = []\n",
    "    detected_objects = []\n",
    "\n",
    "    for index, pts_list in enumerate(cluster_indices):\n",
    "\n",
    "        # Grab the points for the cluster from the extracted outliers (cloud_objects)\n",
    "        pcl_cluster = cloud_objects.extract(pts_list)\n",
    "\n",
    "        # Convert the cluster from pcl to ROS using helper function\n",
    "        ros_cluster = pcl_to_ros(pcl_cluster)\n",
    "\n",
    "        # Extract histogram features\n",
    "        chists = compute_color_histograms(ros_cluster, using_hsv=True)\n",
    "        normals = get_normals(ros_cluster)\n",
    "        nhists = compute_normal_histograms(normals)\n",
    "        feature = np.concatenate((chists, nhists))\n",
    "\n",
    "        # Make the prediction, retrieve the label for the result\n",
    "        # and add it to detected_objects_labels list\n",
    "        prediction = clf.predict(scaler.transform(feature.reshape(1,-1)))\n",
    "        label = encoder.inverse_transform(prediction)[0]\n",
    "        detected_objects_labels.append(label)\n",
    "\n",
    "        # Publish a label into RViz\n",
    "        label_pos = list(white_cloud[pts_list[0]])\n",
    "        label_pos[2] += .4\n",
    "        object_markers_pub.publish(make_label(label,label_pos, index))\n",
    "\n",
    "        # Add the detected object to the list of detected objects.\n",
    "        do = DetectedObject()\n",
    "        do.label = label\n",
    "        do.cloud = ros_cluster\n",
    "        detected_objects.append(do)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![labels.png](images/labels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1 was successful, the pipeline classified all three objects correctly. See the image below for reference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![world-1.png](images/world-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2 was partially successful, the pipeline classified four out of five objects correctly. It misclasifies the book as snacks. See the image below for reference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![world-2.png](images/world-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 3 was successful, the pipeline classified eight out of eight objects correctly. See the image below for reference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![world-3.png](images/world-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the pipeline, I could have used other machine learning algorithms instead of an SVM. For example, I could have used a deep neural network, which may have given better performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could also have used more features for the objects, for example, size of object. More features, may have helped my robot identify objects more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important limitation in this pipeline, is that it requires objects to be seperated. If the glue was lieing on top of the snacks for example, then my clustering algorithm would cluster them together. The resulting features would be different, and the object recognition algorithm wouldn't be able to classify the object, it certainly wouldn't realise there were two objects there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
